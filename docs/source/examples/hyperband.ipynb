{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Search with Hyperband\n",
    "\n",
    "The Hyperband algorithm finds good hyperparameters when using incremental models that receive many chunks of data piece by piece.  It works by trying many parameters on small pieces of data, and then only following up with those parameter sets that seem to be converging quickly.\n",
    "\n",
    "This example simulates searching over two parameters for the sklearn's SGDClassifier, `alpha` and `loss`. These control what objective function we're minimizing and how much regularization is present.\n",
    "\n",
    "Hyperband is similar to `RandomizedSearchCV` and works best with continuous random variables. We simulate log-uniform random variable with lots of samples: `np.logspace(-4, 1, num=1000)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import dask\n",
    "import dask_ml\n",
    "from dask_ml.datasets import make_classification\n",
    "from dask_ml.model_selection import HyperbandCV\n",
    "from dask_ml.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up example problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = 100000, 100\n",
    "X, y = make_classification(n_features=d, \n",
    "                           n_samples=n,\n",
    "                           n_informative=d // 10,\n",
    "                           chunks=(n // 50, d))\n",
    "classes = da.unique(y)\n",
    "X_train, X_test, y_train, y_test = dask.persist(*train_test_split(X, y))\n",
    "\n",
    "model = SGDClassifier(\n",
    "    penalty='elasticnet',\n",
    "    max_iter=1.0, \n",
    "    warm_start=True,\n",
    ")\n",
    "\n",
    "params = {'alpha': np.logspace(-4, 1, num=1000),\n",
    "          'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit quickly with Hyperband\n",
    "\n",
    "The Hyperband algorithm is relatively fast and will find a good set of hyperparameters quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = HyperbandCV(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alg.fit(X_train, y_train, classes=da.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to GridSearchCV\n",
    "\n",
    "We can compare with the traditional GridSearchCV algorithm, which is exhaustive, though comparatively slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import GridSearchCV\n",
    "params = {'alpha': np.logspace(-4, 1, num=100),\n",
    "          'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge']}\n",
    "grid = GridSearchCV(model, params, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid.fit(X, y)  # this may take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results\n",
    "\n",
    "We find that the parameters are not exactly the same, but the results are quite similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(grid.cv_results_)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for loss in df.param_loss.unique():\n",
    "    show = df[df.param_loss == loss]\n",
    "    show.plot(x='param_alpha', y='mean_test_score',\n",
    "              logx=True, ax=ax,\n",
    "             label=loss)\n",
    "plt.plot(2 * [grid.best_params_['alpha']], plt.ylim(), 'p--',\n",
    "         label=f'Hyperband alpha')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('mean_test_score')\n",
    "print('Hyperband loss function =', grid.best_params_['loss'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
