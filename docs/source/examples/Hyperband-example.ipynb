{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperband\n",
    "Hyperband is useful when limited by computational resources. Some examples cases are when\n",
    "\n",
    "* there are many parameters to search over\n",
    "* models take a long time to train\n",
    "\n",
    "Hyperband does only require *one* input, the computational budget. For more information on this, see the documentation: https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html\n",
    "\n",
    "Hyperband is an *adaptive* algorithm: it spends as much time as possible on high-performing models by \"killing\" off the lower portion. More detail in mentioned in the `HyperbandCV` class description: https://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.model_selection.GridSearchCV.html#dask_ml.model_selection.HyperbandCV\n",
    "\n",
    "Below, we'll simulate having many parameters to search over by having one parameters. We would have two, but we want to have a easy-to-interpret visualization at the end.\n",
    "\n",
    "Hyperband is very similar to `RandomizedSearchCV` and works best with continuous random variables. We simulate log-uniform random variable with lots of samples: `np.logspace(-4, 1, num=1000)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import dask_ml\n",
    "from dask_ml.datasets import make_classification\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask_ml.model_selection import HyperbandCV, GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = int(10e3), int(100)\n",
    "X, y = make_classification(n_features=d, n_samples=n,\n",
    "                           n_informative=d // 10,\n",
    "                           chunks=(n // 10, d))\n",
    "classes = da.unique(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "kwargs = dict(penalty='elasticnet', max_iter=1.0, warm_start=True, loss='log')\n",
    "model = Incremental(SGDClassifier(**kwargs))\n",
    "params = {'alpha': np.logspace(-4, 1, num=1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = HyperbandCV(model, params, max_iter=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "alg.fit(X_train, y_train, classes=da.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperband_alpha = alg.best_params_['alpha']\n",
    "alg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now will we compare with an exhaustive evaluation, which we can do because we're only simulating being computationally limited.\n",
    "\n",
    "We will use `GridSearchCV`, and set the loss of the model to be the loss Hyperband found. We'll do this because this is the really the only to show an fair visualization: otherwise we're comparing `alpha`s across loss functions, which doesn't make sense.\n",
    "\n",
    "Note that this visualization hides the fact that Hyperband was searching between 5 different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'alpha': np.logspace(-4, 1, num=50)}\n",
    "grid = GridSearchCV(model.estimator, params, return_train_score=False)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "df.plot(x='param_alpha', y='mean_test_score',\n",
    "        yerr='std_test_score',\n",
    "        logx=True, ax=ax)\n",
    "ax.plot(2 * [hyperband_alpha], plt.ylim(), 'r--',\n",
    "         label=\"Hyperband's chosen alpha\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylabel('mean_test_score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
